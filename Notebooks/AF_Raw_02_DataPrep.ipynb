{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da50704-9b49-4ff5-bb12-b91c0731abef",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Project - Data Pre-processing (Raw Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f4dce-64a0-4634-87b4-7d942522267a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Student Name: Michael (s2767708) & Pedro da Silva (s2799057)\n",
    "Group No: 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba34513-7e6a-485e-a816-918cd889b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import sweetviz as sv\n",
    "from tqdm import tqdm, trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppress the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print('Imported all libraries successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfcf1ce-3359-46dc-9942-a25fe0962e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c851cd5-7152-4f8f-a547-fafccae64ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the raw txt files\n",
    "def read_files(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(str, line)) for line in data]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff670f4-41ec-494c-ba84-c88f1cbd5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pre-process the raw datasets\n",
    "\n",
    "def preproc_files(ecg_file_path, control_file_path):\n",
    "    \n",
    "    # Read ECG file\n",
    "    ecg_file = read_files(ecg_file_path)\n",
    "    \n",
    "    # Convert the list into a DataFrame\n",
    "    ecg_df = pd.DataFrame(ecg_file)\n",
    "    \n",
    "    # Create a single \"annotation\" column to join all annotations\n",
    "    ecg_df['annotation'] = ecg_df[ecg_df.columns[3:]].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "    \n",
    "    # Drop the old annotation columns, keep the merged annotation column in the last position\n",
    "    ecg_df.drop(ecg_df.iloc[:,3:(len(ecg_df.columns)-1)], axis=1, inplace=True)\n",
    "    \n",
    "    # Trim the white spaces in the annotation column\n",
    "    ecg_df['annotation'] = ecg_df['annotation'].str.strip()\n",
    "\n",
    "    # Rename the final columns\n",
    "    col_name = ['time', 'rr_interval', 'heartbeat_type', 'annotation']\n",
    "    ecg_df.columns = col_name\n",
    "    ecg_df['rr_interval'] = ecg_df['rr_interval'].astype(int)\n",
    "    \n",
    "    # Keep the rows where there are no annotations\n",
    "    ecg_df = ecg_df[ecg_df['annotation']=='']\n",
    "    \n",
    "    # Handle outliers\n",
    "    ecg_df = ecg_df.loc[(ecg_df['rr_interval'] >= 100) & (ecg_df['rr_interval'] <= 1600)]\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    ecg_df_preproc = ecg_df.drop(['heartbeat_type','annotation'], axis=1)\n",
    "    \n",
    "    # Drop duplicates, if any\n",
    "    ecg_df_preproc.drop_duplicates()\n",
    "    \n",
    "    # Read Control file\n",
    "    control_file = read_files(control_file_path)\n",
    "    \n",
    "    # Convert the list into a DataFrame\n",
    "    control_df = pd.DataFrame(control_file)\n",
    "    \n",
    "    # Rename the final columns\n",
    "    col_name = ['time', 'af']\n",
    "    control_df.columns = col_name\n",
    "    \n",
    "    # Delete records with Control = -1\n",
    "    control_df = control_df[control_df['af']!='-1']\n",
    "    \n",
    "    # Convert time column into H:M:S format\n",
    "    control_df['time'] = control_df['time'].str[:- 4]\n",
    "\n",
    "    # Convert into time dtypes\n",
    "    control_df['time'] = pd.to_datetime(control_df['time'], format='%H:%M:%S')\n",
    "    \n",
    "    # Generate 30s time stamp for df_label_new\n",
    "    control_df['time_30'] = control_df['time'].map(lambda x: pd.date_range(start=x, periods=30, freq='s'))\n",
    "    \n",
    "    # Explode the time_30 column\n",
    "    control_df = control_df.explode('time_30')\n",
    "    control_df['time_30'] = control_df['time_30'].dt.time # Get the H:M:S format\n",
    "    control_df['time_30'] = control_df['time_30'].astype(str) # Convert to string for inner join with ECG data\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    control_df_preproc = control_df.drop(['time'], axis=1)\n",
    "    \n",
    "    # Drop duplicates, if any\n",
    "    control_df_preproc.drop_duplicates()\n",
    "    \n",
    "    # Merge ECG & Control data\n",
    "    ecg_control_df = pd.merge(ecg_df_preproc, control_df_preproc,\n",
    "                              how='inner', left_on='time', right_on='time_30',\n",
    "                              left_index=False, right_index=False)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    ecg_control_df = ecg_control_df.drop(['time_30'], axis=1)\n",
    "\n",
    "    # Drop duplicates, if any\n",
    "    ecg_control_df.drop_duplicates()\n",
    "\n",
    "    # Reset index\n",
    "    ecg_control_df = ecg_control_df.reset_index(drop=True)\n",
    "    \n",
    "    # Creating overlaping windows of 30 secs and 50% overlap\n",
    "    rr_interval_list = []\n",
    "    af_list = []\n",
    "    window_size = 30\n",
    "    step_size = 15    \n",
    "    for i in range(0, ecg_control_df.shape[0]-window_size, step_size):\n",
    "        rr_intervals = ecg_control_df['rr_interval'].values[i:i+window_size]\n",
    "        afs = stats.mode(ecg_control_df['af'][i:i+window_size])[0][0]\n",
    "        rr_interval_list.append(rr_intervals)\n",
    "        af_list.append(afs)\n",
    "    \n",
    "    # Convert the lists into a DataFrame\n",
    "    rr_af_df = pd.DataFrame({'rr_intervals': rr_interval_list, 'af': af_list})\n",
    "    \n",
    "    return rr_af_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075fd4ca-f008-41f1-affa-fcd2f9383a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 ecg_txt files:\n",
      " ['C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data1.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data2.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data3.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data4.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data5.txt']\n",
      "\n",
      "Last 5 ecg_txt files:\n",
      " ['C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data800.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data801.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data802.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data803.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\ECG_Data\\\\Data804.txt']\n"
     ]
    }
   ],
   "source": [
    "# Take all ECG file paths in the local directory\n",
    "ecg_txt_paths = glob.glob(current_path + '\\\\dataset\\\\ECG_Data\\\\Data*.txt')\n",
    "ecg_txt_paths = sorted(ecg_txt_paths, key = lambda x: int(x.split(\"/\")[-1].split(\".\")[0].split(\"Data\")[-1])) # sort by the integer of filenames\n",
    "print('First 5 ecg_txt files:\\n', ecg_txt_paths[0:5])\n",
    "print('\\nLast 5 ecg_txt files:\\n', ecg_txt_paths[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90833d43-b06e-463c-babf-6cc0b8d9afb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 control_txt files:\n",
      " ['C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control1.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control2.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control3.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control4.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control5.txt']\n",
      "\n",
      "Last 5 control_txt files:\n",
      " ['C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control800.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control801.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control802.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control803.txt', 'C:\\\\Users\\\\HP\\\\Desktop\\\\MASTER\\\\UniversityofTwente\\\\Master\\\\202302 Q2\\\\Data Science\\\\Project\\\\AF\\\\dataset\\\\Class\\\\Control804.txt']\n"
     ]
    }
   ],
   "source": [
    "# Take all Control file paths in the local directory\n",
    "control_txt_paths = glob.glob(current_path + '\\\\dataset\\\\Class\\\\Control*.txt')\n",
    "control_txt_paths = sorted(control_txt_paths, key = lambda x: int(x.split(\"/\")[-1].split(\".\")[0].split(\"Control\")[-1])) # sort by the integer of filenames\n",
    "print('First 5 control_txt files:\\n', control_txt_paths[0:5])\n",
    "print('\\nLast 5 control_txt files:\\n', control_txt_paths[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a7006d-93d3-4f79-8339-33ed7fa0cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:38:12<00:00, 29.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1049997, 2)\n",
      "\n",
      "Number of AF cases:\n",
      "0    1016245\n",
      "1      33752\n",
      "Name: af, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rr_intervals</th>\n",
       "      <th>af</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[535, 765, 780, 775, 775, 765, 770, 775, 770, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[775, 790, 785, 780, 785, 795, 795, 795, 785, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[785, 795, 795, 785, 785, 790, 790, 795, 790, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[790, 775, 780, 780, 790, 795, 805, 785, 785, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[795, 800, 805, 800, 795, 775, 790, 785, 790, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        rr_intervals af\n",
       "0  [535, 765, 780, 775, 775, 765, 770, 775, 770, ...  0\n",
       "1  [775, 790, 785, 780, 785, 795, 795, 795, 785, ...  0\n",
       "2  [785, 795, 795, 785, 785, 790, 790, 795, 790, ...  0\n",
       "3  [790, 775, 780, 780, 790, 795, 805, 785, 785, ...  0\n",
       "4  [795, 800, 805, 800, 795, 775, 790, 785, 790, ...  0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_files_to_read = 200 # CHANGE IF NECESSARY!!!\n",
    "\n",
    "af_df_preproc = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0, num_files_to_read)):\n",
    "    # tqdm(range(0, num_files_to_read)).set_description(f'Processing Data: {i+1} of {num_files_to_read}')\n",
    "    rr_af_df_part = preproc_files(ecg_txt_paths[i], control_txt_paths[i])\n",
    "    af_df_preproc = af_df_preproc.append(rr_af_df_part)\n",
    "\n",
    "print(af_df_preproc.shape)\n",
    "print('\\nNumber of AF cases:')\n",
    "print(af_df_preproc['af'].value_counts(), '\\n')\n",
    "af_df_preproc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01430ad7-c687-4d68-a7b3-442b69387208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "af_df_preproc.to_parquet(current_path + '\\\\af_df_preproc_' + str(num_files_to_read) + '.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
